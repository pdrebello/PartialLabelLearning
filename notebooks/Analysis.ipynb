{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_folder = \"/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/cc_loss_Adam_0.1_1e-06/0', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/cc_loss_Adam_0.1_1e-06/1', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/cc_loss_Adam_0.1_1e-06/2', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/cc_loss_Adam_0.1_1e-06/3', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/cc_loss_Adam_0.1_1e-06/4', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/cc_loss_Adam_0.1_1e-06/6', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/cc_loss_Adam_0.1_1e-06/9', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/cc_loss_Adam_0.1_1e-06/7', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/cc_loss_Adam_0.1_1e-06/5', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/cc_loss_Adam_0.1_1e-06/8', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/weighted_loss_xy_lstm_iexplr_Adam_0.01_1e-06_100_pretrain/0', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/weighted_loss_xy_lstm_iexplr_Adam_0.01_1e-06_100_pretrain/1', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/weighted_loss_xy_lstm_iexplr_Adam_0.01_1e-06_100_pretrain/2', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/weighted_loss_xy_lstm_iexplr_Adam_0.01_1e-06_100_pretrain/3', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/weighted_loss_xy_lstm_iexplr_Adam_0.01_1e-06_100_pretrain/4', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/weighted_loss_xy_lstm_iexplr_Adam_0.01_1e-06_100_pretrain/6', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/weighted_loss_xy_lstm_iexplr_Adam_0.01_1e-06_100_pretrain/9', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/weighted_loss_xy_lstm_iexplr_Adam_0.01_1e-06_100_pretrain/7', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/weighted_loss_xy_lstm_iexplr_Adam_0.01_1e-06_100_pretrain/5', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/weighted_loss_xy_lstm_iexplr_Adam_0.01_1e-06_100_pretrain/8', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/cc_loss_Adam_0.01_1e-06/0', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/cc_loss_Adam_0.01_1e-06/1', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/cc_loss_Adam_0.01_1e-06/2', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/cc_loss_Adam_0.01_1e-06/3', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/cc_loss_Adam_0.01_1e-06/4', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/cc_loss_Adam_0.01_1e-06/6', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/cc_loss_Adam_0.01_1e-06/9', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/cc_loss_Adam_0.01_1e-06/7', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/cc_loss_Adam_0.01_1e-06/5', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/cc_loss_Adam_0.01_1e-06/8', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/weighted_loss_xy_lstm_iexplr_Adam_0.1_1e-06_100_pretrain/0', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/weighted_loss_xy_lstm_iexplr_Adam_0.1_1e-06_100_pretrain/1', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/weighted_loss_xy_lstm_iexplr_Adam_0.1_1e-06_100_pretrain/2', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/weighted_loss_xy_lstm_iexplr_Adam_0.1_1e-06_100_pretrain/3', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/weighted_loss_xy_lstm_iexplr_Adam_0.1_1e-06_100_pretrain/4', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/weighted_loss_xy_lstm_iexplr_Adam_0.1_1e-06_100_pretrain/6', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/weighted_loss_xy_lstm_iexplr_Adam_0.1_1e-06_100_pretrain/9', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/weighted_loss_xy_lstm_iexplr_Adam_0.1_1e-06_100_pretrain/7', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/weighted_loss_xy_lstm_iexplr_Adam_0.1_1e-06_100_pretrain/5', '/home/yatin/hpchome/pratheek/PartialLabelLearning/results/lstm_flip/MSRCv2_flip/3layer/weighted_loss_xy_lstm_iexplr_Adam_0.1_1e-06_100_pretrain/8']\n"
     ]
    }
   ],
   "source": [
    "import os,string\n",
    "path = result_folder\n",
    "path = os.path.normpath(path)\n",
    "res = []\n",
    "for root,dirs,files in os.walk(path, topdown=True):\n",
    "    depth = root[len(path) + len(os.path.sep):].count(os.path.sep)\n",
    "    if depth == 2:\n",
    "        # We're currently two directories in, so all subdirs have depth 3\n",
    "        res += [os.path.join(root, d) for d in dirs]\n",
    "        dirs[:] = [] # Don't recurse any deeper\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['dataset','technique','fold','real_test_acc','train_IOU','train_IOU_neg','val_IOU','val_IOU_neg','test_IOU','test_IOU_neg','surrogate_test_acc', 'surrogate_train_acc', \n",
    "'real_train_acc', 'surrogate_val_acc', 'real_val_acc',  'train_confidence', 'val_confidence', 'test_confidence','best_epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time \n",
    "data = pd.DataFrame(columns=columns) \n",
    "count = 0\n",
    "for folder in res:\n",
    "    \n",
    "    logfile = os.path.join(folder, \"logs\",\"log.json\")\n",
    "    if(not(os.path.exists(logfile))):\n",
    "        continue\n",
    "        \n",
    "    #if(\"cc_loss\" in logfile):\n",
    "    if(\"0.1\" in logfile):\n",
    "        continue\n",
    "    #if(\"cc_loss\" in logfile):\n",
    "    #    if(\"1_1e-05\" in logfile):\n",
    "    #        continue\n",
    "    #if(not(\"/2/\" in logfile)):\n",
    "    #    continue\n",
    "    #if(not(\"cc_loss\") in logfile):\n",
    "    #    if(not(\"pretrain\") in logfile):\n",
    "    #        continue\n",
    "    \n",
    "    count+=1\n",
    "    #print(\"created: %s\" % time.ctime(os.path.getctime(logfile)))\n",
    "    df = pd.read_json(logfile, lines=True)\n",
    "    df = df[df[\"epoch\"] == -1]\n",
    "    #print(df['real_test_acc'])\n",
    "    #print(df.columns.values)\n",
    "    names = folder.split(\"/\")\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        if(row[\"epoch\"] == -1):\n",
    "            data.loc[count] = row\n",
    "            data.loc[count,\"dataset\"] = names[-4]\n",
    "            #data.loc[count,\"model\"] = names[-3]\n",
    "            data.loc[count,\"technique\"] = names[-2]\n",
    "            data.loc[count,\"fold\"] = names[-1]\n",
    "            \n",
    "            #print(data)\n",
    "print(count)\n",
    "#data.to_csv('fold_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320\n",
      "320\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "techniques = data[[\"technique\",\"fold\",\"dataset\"]].values.tolist()\n",
    "\n",
    "dic = {}\n",
    "techs = 0\n",
    "for i in techniques:\n",
    "    if((\"lstm\" in i[0]) and (\"detach\" in i[0]) and (\"MSRCv2_lstm\") in i[2]):\n",
    "        techs+=1\n",
    "        tokens = i[0].split(\"_\")\n",
    "        \n",
    "        wd = tokens[-3]\n",
    "        lr = tokens[-4]\n",
    "        fold = i[1]\n",
    "        opt = tokens[-5]\n",
    "        pretrain = tokens[-2]\n",
    "        dic[(opt,lr,wd,fold,pretrain)] = 1\n",
    "count = 0\n",
    "missing = 0\n",
    "for lr in ['0.01','0.1']:\n",
    "    for wd in [0.001, 0.0001, 1e-05, 1e-06]:\n",
    "        for opt in ['Adam','SGD']:\n",
    "            for fold in range(10):\n",
    "                for pretrain in [0,100]:\n",
    "                    count+=1\n",
    "                    if(not((opt,str(lr),str(wd),str(fold),str(pretrain)) in dic)):\n",
    "                        #print(\"weighted_loss_xy_lstm_{}_{}_{}_{}_NONORM/{}\".format(opt,lr,wd,pretrain,fold))\n",
    "                        missing +=1\n",
    "print(count)\n",
    "print(missing)\n",
    "print(techs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>real_test_acc</th>\n",
       "      <th>train_IOU</th>\n",
       "      <th>val_IOU</th>\n",
       "      <th>test_IOU</th>\n",
       "      <th>surrogate_test_acc</th>\n",
       "      <th>surrogate_train_acc</th>\n",
       "      <th>real_train_acc</th>\n",
       "      <th>surrogate_val_acc</th>\n",
       "      <th>real_val_acc</th>\n",
       "      <th>train_confidence</th>\n",
       "      <th>val_confidence</th>\n",
       "      <th>test_confidence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th>technique</th>\n",
       "      <th>fold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">BirdSong</th>\n",
       "      <th>cc_loss_Adam_0.0001_0.001</th>\n",
       "      <th>0</th>\n",
       "      <td>73.146293</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82.765531</td>\n",
       "      <td>90.355711</td>\n",
       "      <td>78.507014</td>\n",
       "      <td>81.963928</td>\n",
       "      <td>71.743487</td>\n",
       "      <td>0.804089</td>\n",
       "      <td>0.797763</td>\n",
       "      <td>0.807049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cc_loss_Adam_0.0001_1e-05</th>\n",
       "      <th>0</th>\n",
       "      <td>72.945892</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82.765531</td>\n",
       "      <td>90.330661</td>\n",
       "      <td>78.431864</td>\n",
       "      <td>81.963928</td>\n",
       "      <td>71.743487</td>\n",
       "      <td>0.805712</td>\n",
       "      <td>0.799697</td>\n",
       "      <td>0.808707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cc_loss_Adam_0.001_0.001</th>\n",
       "      <th>0</th>\n",
       "      <td>73.346693</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82.765531</td>\n",
       "      <td>92.384770</td>\n",
       "      <td>78.156313</td>\n",
       "      <td>81.963928</td>\n",
       "      <td>70.741483</td>\n",
       "      <td>0.913172</td>\n",
       "      <td>0.905254</td>\n",
       "      <td>0.893597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cc_loss_Adam_0.001_1e-06</th>\n",
       "      <th>0</th>\n",
       "      <td>71.743487</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81.963928</td>\n",
       "      <td>87.299599</td>\n",
       "      <td>75.225451</td>\n",
       "      <td>81.563126</td>\n",
       "      <td>70.340681</td>\n",
       "      <td>0.822117</td>\n",
       "      <td>0.825652</td>\n",
       "      <td>0.822136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cc_loss_Adam_0.01_0.0001</th>\n",
       "      <th>0</th>\n",
       "      <td>74.348697</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.767535</td>\n",
       "      <td>88.001002</td>\n",
       "      <td>75.275551</td>\n",
       "      <td>82.164329</td>\n",
       "      <td>70.541082</td>\n",
       "      <td>0.849635</td>\n",
       "      <td>0.843747</td>\n",
       "      <td>0.854545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">lost_noisy_annotator_p</th>\n",
       "      <th>weighted_loss_xy_lstm_SGD_0.0001_1e-05</th>\n",
       "      <th>0</th>\n",
       "      <td>51.785714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63.392857</td>\n",
       "      <td>89.285714</td>\n",
       "      <td>68.638393</td>\n",
       "      <td>80.357143</td>\n",
       "      <td>68.750000</td>\n",
       "      <td>0.573889</td>\n",
       "      <td>0.518727</td>\n",
       "      <td>0.487337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted_loss_xy_lstm_SGD_0.001_1e-06</th>\n",
       "      <th>0</th>\n",
       "      <td>66.071429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76.785714</td>\n",
       "      <td>99.107143</td>\n",
       "      <td>82.477679</td>\n",
       "      <td>84.821429</td>\n",
       "      <td>73.214286</td>\n",
       "      <td>0.857294</td>\n",
       "      <td>0.790359</td>\n",
       "      <td>0.715717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted_loss_xy_lstm_SGD_0.1_0.0001</th>\n",
       "      <th>0</th>\n",
       "      <td>68.750000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>89.620536</td>\n",
       "      <td>83.928571</td>\n",
       "      <td>74.107143</td>\n",
       "      <td>0.992276</td>\n",
       "      <td>0.916223</td>\n",
       "      <td>0.876607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted_loss_xy_lstm_SGD_0.1_1e-05</th>\n",
       "      <th>0</th>\n",
       "      <td>66.071429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.892857</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>89.732143</td>\n",
       "      <td>83.928571</td>\n",
       "      <td>75.892857</td>\n",
       "      <td>0.996020</td>\n",
       "      <td>0.918119</td>\n",
       "      <td>0.887742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted_loss_xy_lstm_SGD_0.1_1e-06</th>\n",
       "      <th>0</th>\n",
       "      <td>68.750000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78.571429</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>89.397321</td>\n",
       "      <td>83.928571</td>\n",
       "      <td>75.892857</td>\n",
       "      <td>0.991881</td>\n",
       "      <td>0.923484</td>\n",
       "      <td>0.885091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2329 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    real_test_acc  \\\n",
       "dataset                technique                              fold                  \n",
       "BirdSong               cc_loss_Adam_0.0001_0.001              0         73.146293   \n",
       "                       cc_loss_Adam_0.0001_1e-05              0         72.945892   \n",
       "                       cc_loss_Adam_0.001_0.001               0         73.346693   \n",
       "                       cc_loss_Adam_0.001_1e-06               0         71.743487   \n",
       "                       cc_loss_Adam_0.01_0.0001               0         74.348697   \n",
       "...                                                                           ...   \n",
       "lost_noisy_annotator_p weighted_loss_xy_lstm_SGD_0.0001_1e-05 0         51.785714   \n",
       "                       weighted_loss_xy_lstm_SGD_0.001_1e-06  0         66.071429   \n",
       "                       weighted_loss_xy_lstm_SGD_0.1_0.0001   0         68.750000   \n",
       "                       weighted_loss_xy_lstm_SGD_0.1_1e-05    0         66.071429   \n",
       "                       weighted_loss_xy_lstm_SGD_0.1_1e-06    0         68.750000   \n",
       "\n",
       "                                                                    train_IOU  \\\n",
       "dataset                technique                              fold              \n",
       "BirdSong               cc_loss_Adam_0.0001_0.001              0           NaN   \n",
       "                       cc_loss_Adam_0.0001_1e-05              0           NaN   \n",
       "                       cc_loss_Adam_0.001_0.001               0           NaN   \n",
       "                       cc_loss_Adam_0.001_1e-06               0           NaN   \n",
       "                       cc_loss_Adam_0.01_0.0001               0           NaN   \n",
       "...                                                                       ...   \n",
       "lost_noisy_annotator_p weighted_loss_xy_lstm_SGD_0.0001_1e-05 0           NaN   \n",
       "                       weighted_loss_xy_lstm_SGD_0.001_1e-06  0           NaN   \n",
       "                       weighted_loss_xy_lstm_SGD_0.1_0.0001   0           NaN   \n",
       "                       weighted_loss_xy_lstm_SGD_0.1_1e-05    0           NaN   \n",
       "                       weighted_loss_xy_lstm_SGD_0.1_1e-06    0           NaN   \n",
       "\n",
       "                                                                    val_IOU  \\\n",
       "dataset                technique                              fold            \n",
       "BirdSong               cc_loss_Adam_0.0001_0.001              0         NaN   \n",
       "                       cc_loss_Adam_0.0001_1e-05              0         NaN   \n",
       "                       cc_loss_Adam_0.001_0.001               0         NaN   \n",
       "                       cc_loss_Adam_0.001_1e-06               0         NaN   \n",
       "                       cc_loss_Adam_0.01_0.0001               0         NaN   \n",
       "...                                                                     ...   \n",
       "lost_noisy_annotator_p weighted_loss_xy_lstm_SGD_0.0001_1e-05 0         NaN   \n",
       "                       weighted_loss_xy_lstm_SGD_0.001_1e-06  0         NaN   \n",
       "                       weighted_loss_xy_lstm_SGD_0.1_0.0001   0         NaN   \n",
       "                       weighted_loss_xy_lstm_SGD_0.1_1e-05    0         NaN   \n",
       "                       weighted_loss_xy_lstm_SGD_0.1_1e-06    0         NaN   \n",
       "\n",
       "                                                                    test_IOU  \\\n",
       "dataset                technique                              fold             \n",
       "BirdSong               cc_loss_Adam_0.0001_0.001              0          NaN   \n",
       "                       cc_loss_Adam_0.0001_1e-05              0          NaN   \n",
       "                       cc_loss_Adam_0.001_0.001               0          NaN   \n",
       "                       cc_loss_Adam_0.001_1e-06               0          NaN   \n",
       "                       cc_loss_Adam_0.01_0.0001               0          NaN   \n",
       "...                                                                      ...   \n",
       "lost_noisy_annotator_p weighted_loss_xy_lstm_SGD_0.0001_1e-05 0          NaN   \n",
       "                       weighted_loss_xy_lstm_SGD_0.001_1e-06  0          NaN   \n",
       "                       weighted_loss_xy_lstm_SGD_0.1_0.0001   0          NaN   \n",
       "                       weighted_loss_xy_lstm_SGD_0.1_1e-05    0          NaN   \n",
       "                       weighted_loss_xy_lstm_SGD_0.1_1e-06    0          NaN   \n",
       "\n",
       "                                                                    surrogate_test_acc  \\\n",
       "dataset                technique                              fold                       \n",
       "BirdSong               cc_loss_Adam_0.0001_0.001              0              82.765531   \n",
       "                       cc_loss_Adam_0.0001_1e-05              0              82.765531   \n",
       "                       cc_loss_Adam_0.001_0.001               0              82.765531   \n",
       "                       cc_loss_Adam_0.001_1e-06               0              81.963928   \n",
       "                       cc_loss_Adam_0.01_0.0001               0              83.767535   \n",
       "...                                                                                ...   \n",
       "lost_noisy_annotator_p weighted_loss_xy_lstm_SGD_0.0001_1e-05 0              63.392857   \n",
       "                       weighted_loss_xy_lstm_SGD_0.001_1e-06  0              76.785714   \n",
       "                       weighted_loss_xy_lstm_SGD_0.1_0.0001   0              75.000000   \n",
       "                       weighted_loss_xy_lstm_SGD_0.1_1e-05    0              75.892857   \n",
       "                       weighted_loss_xy_lstm_SGD_0.1_1e-06    0              78.571429   \n",
       "\n",
       "                                                                    surrogate_train_acc  \\\n",
       "dataset                technique                              fold                        \n",
       "BirdSong               cc_loss_Adam_0.0001_0.001              0               90.355711   \n",
       "                       cc_loss_Adam_0.0001_1e-05              0               90.330661   \n",
       "                       cc_loss_Adam_0.001_0.001               0               92.384770   \n",
       "                       cc_loss_Adam_0.001_1e-06               0               87.299599   \n",
       "                       cc_loss_Adam_0.01_0.0001               0               88.001002   \n",
       "...                                                                                 ...   \n",
       "lost_noisy_annotator_p weighted_loss_xy_lstm_SGD_0.0001_1e-05 0               89.285714   \n",
       "                       weighted_loss_xy_lstm_SGD_0.001_1e-06  0               99.107143   \n",
       "                       weighted_loss_xy_lstm_SGD_0.1_0.0001   0              100.000000   \n",
       "                       weighted_loss_xy_lstm_SGD_0.1_1e-05    0              100.000000   \n",
       "                       weighted_loss_xy_lstm_SGD_0.1_1e-06    0              100.000000   \n",
       "\n",
       "                                                                    real_train_acc  \\\n",
       "dataset                technique                              fold                   \n",
       "BirdSong               cc_loss_Adam_0.0001_0.001              0          78.507014   \n",
       "                       cc_loss_Adam_0.0001_1e-05              0          78.431864   \n",
       "                       cc_loss_Adam_0.001_0.001               0          78.156313   \n",
       "                       cc_loss_Adam_0.001_1e-06               0          75.225451   \n",
       "                       cc_loss_Adam_0.01_0.0001               0          75.275551   \n",
       "...                                                                            ...   \n",
       "lost_noisy_annotator_p weighted_loss_xy_lstm_SGD_0.0001_1e-05 0          68.638393   \n",
       "                       weighted_loss_xy_lstm_SGD_0.001_1e-06  0          82.477679   \n",
       "                       weighted_loss_xy_lstm_SGD_0.1_0.0001   0          89.620536   \n",
       "                       weighted_loss_xy_lstm_SGD_0.1_1e-05    0          89.732143   \n",
       "                       weighted_loss_xy_lstm_SGD_0.1_1e-06    0          89.397321   \n",
       "\n",
       "                                                                    surrogate_val_acc  \\\n",
       "dataset                technique                              fold                      \n",
       "BirdSong               cc_loss_Adam_0.0001_0.001              0             81.963928   \n",
       "                       cc_loss_Adam_0.0001_1e-05              0             81.963928   \n",
       "                       cc_loss_Adam_0.001_0.001               0             81.963928   \n",
       "                       cc_loss_Adam_0.001_1e-06               0             81.563126   \n",
       "                       cc_loss_Adam_0.01_0.0001               0             82.164329   \n",
       "...                                                                               ...   \n",
       "lost_noisy_annotator_p weighted_loss_xy_lstm_SGD_0.0001_1e-05 0             80.357143   \n",
       "                       weighted_loss_xy_lstm_SGD_0.001_1e-06  0             84.821429   \n",
       "                       weighted_loss_xy_lstm_SGD_0.1_0.0001   0             83.928571   \n",
       "                       weighted_loss_xy_lstm_SGD_0.1_1e-05    0             83.928571   \n",
       "                       weighted_loss_xy_lstm_SGD_0.1_1e-06    0             83.928571   \n",
       "\n",
       "                                                                    real_val_acc  \\\n",
       "dataset                technique                              fold                 \n",
       "BirdSong               cc_loss_Adam_0.0001_0.001              0        71.743487   \n",
       "                       cc_loss_Adam_0.0001_1e-05              0        71.743487   \n",
       "                       cc_loss_Adam_0.001_0.001               0        70.741483   \n",
       "                       cc_loss_Adam_0.001_1e-06               0        70.340681   \n",
       "                       cc_loss_Adam_0.01_0.0001               0        70.541082   \n",
       "...                                                                          ...   \n",
       "lost_noisy_annotator_p weighted_loss_xy_lstm_SGD_0.0001_1e-05 0        68.750000   \n",
       "                       weighted_loss_xy_lstm_SGD_0.001_1e-06  0        73.214286   \n",
       "                       weighted_loss_xy_lstm_SGD_0.1_0.0001   0        74.107143   \n",
       "                       weighted_loss_xy_lstm_SGD_0.1_1e-05    0        75.892857   \n",
       "                       weighted_loss_xy_lstm_SGD_0.1_1e-06    0        75.892857   \n",
       "\n",
       "                                                                    train_confidence  \\\n",
       "dataset                technique                              fold                     \n",
       "BirdSong               cc_loss_Adam_0.0001_0.001              0             0.804089   \n",
       "                       cc_loss_Adam_0.0001_1e-05              0             0.805712   \n",
       "                       cc_loss_Adam_0.001_0.001               0             0.913172   \n",
       "                       cc_loss_Adam_0.001_1e-06               0             0.822117   \n",
       "                       cc_loss_Adam_0.01_0.0001               0             0.849635   \n",
       "...                                                                              ...   \n",
       "lost_noisy_annotator_p weighted_loss_xy_lstm_SGD_0.0001_1e-05 0             0.573889   \n",
       "                       weighted_loss_xy_lstm_SGD_0.001_1e-06  0             0.857294   \n",
       "                       weighted_loss_xy_lstm_SGD_0.1_0.0001   0             0.992276   \n",
       "                       weighted_loss_xy_lstm_SGD_0.1_1e-05    0             0.996020   \n",
       "                       weighted_loss_xy_lstm_SGD_0.1_1e-06    0             0.991881   \n",
       "\n",
       "                                                                    val_confidence  \\\n",
       "dataset                technique                              fold                   \n",
       "BirdSong               cc_loss_Adam_0.0001_0.001              0           0.797763   \n",
       "                       cc_loss_Adam_0.0001_1e-05              0           0.799697   \n",
       "                       cc_loss_Adam_0.001_0.001               0           0.905254   \n",
       "                       cc_loss_Adam_0.001_1e-06               0           0.825652   \n",
       "                       cc_loss_Adam_0.01_0.0001               0           0.843747   \n",
       "...                                                                            ...   \n",
       "lost_noisy_annotator_p weighted_loss_xy_lstm_SGD_0.0001_1e-05 0           0.518727   \n",
       "                       weighted_loss_xy_lstm_SGD_0.001_1e-06  0           0.790359   \n",
       "                       weighted_loss_xy_lstm_SGD_0.1_0.0001   0           0.916223   \n",
       "                       weighted_loss_xy_lstm_SGD_0.1_1e-05    0           0.918119   \n",
       "                       weighted_loss_xy_lstm_SGD_0.1_1e-06    0           0.923484   \n",
       "\n",
       "                                                                    test_confidence  \n",
       "dataset                technique                              fold                   \n",
       "BirdSong               cc_loss_Adam_0.0001_0.001              0            0.807049  \n",
       "                       cc_loss_Adam_0.0001_1e-05              0            0.808707  \n",
       "                       cc_loss_Adam_0.001_0.001               0            0.893597  \n",
       "                       cc_loss_Adam_0.001_1e-06               0            0.822136  \n",
       "                       cc_loss_Adam_0.01_0.0001               0            0.854545  \n",
       "...                                                                             ...  \n",
       "lost_noisy_annotator_p weighted_loss_xy_lstm_SGD_0.0001_1e-05 0            0.487337  \n",
       "                       weighted_loss_xy_lstm_SGD_0.001_1e-06  0            0.715717  \n",
       "                       weighted_loss_xy_lstm_SGD_0.1_0.0001   0            0.876607  \n",
       "                       weighted_loss_xy_lstm_SGD_0.1_1e-05    0            0.887742  \n",
       "                       weighted_loss_xy_lstm_SGD_0.1_1e-06    0            0.885091  \n",
       "\n",
       "[2329 rows x 12 columns]"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[columns].groupby(['dataset','technique','fold']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_sort(tup):\n",
    "    #print(tup)\n",
    "    print(tup)\n",
    "    if('lost' in tup[1]):\n",
    "        a = 40\n",
    "    elif('MSRCv2' in tup[1]):\n",
    "        a = 10\n",
    "    elif('BirdSong' in tup[1]):\n",
    "        a = 20\n",
    "    else:\n",
    "        a = 30\n",
    "        \n",
    "    if('one' in tup[1]):\n",
    "        a += 1\n",
    "    elif('two' in tup[1]):\n",
    "        a += 2\n",
    "    elif('flip' in tup[1]):\n",
    "        a += 3\n",
    "    elif('noisy' in tup[1]):\n",
    "        a += 4\n",
    "    else:\n",
    "        a += 5\n",
    "    return ( a,tup[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>technique</th>\n",
       "      <th>fold</th>\n",
       "      <th>real_test_acc</th>\n",
       "      <th>train_IOU</th>\n",
       "      <th>train_IOU_neg</th>\n",
       "      <th>val_IOU</th>\n",
       "      <th>val_IOU_neg</th>\n",
       "      <th>test_IOU</th>\n",
       "      <th>test_IOU_neg</th>\n",
       "      <th>surrogate_test_acc</th>\n",
       "      <th>surrogate_train_acc</th>\n",
       "      <th>real_train_acc</th>\n",
       "      <th>surrogate_val_acc</th>\n",
       "      <th>real_val_acc</th>\n",
       "      <th>train_confidence</th>\n",
       "      <th>val_confidence</th>\n",
       "      <th>test_confidence</th>\n",
       "      <th>best_epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lost_noisy_annotator_p</td>\n",
       "      <td>cc_loss_Adam_0.001_0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>71.428571</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82.142857</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>87.946429</td>\n",
       "      <td>86.607143</td>\n",
       "      <td>80.357143</td>\n",
       "      <td>0.996920</td>\n",
       "      <td>0.917440</td>\n",
       "      <td>0.875374</td>\n",
       "      <td>1271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>lost_noisy_annotator_p</td>\n",
       "      <td>cc_loss_Adam_0.001_1e-05</td>\n",
       "      <td>0</td>\n",
       "      <td>67.857143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77.678571</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>87.834821</td>\n",
       "      <td>86.607143</td>\n",
       "      <td>82.142857</td>\n",
       "      <td>0.997555</td>\n",
       "      <td>0.928105</td>\n",
       "      <td>0.908326</td>\n",
       "      <td>1391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>lost_noisy_annotator_p</td>\n",
       "      <td>cc_loss_SGD_0.0001_1e-05</td>\n",
       "      <td>0</td>\n",
       "      <td>40.178571</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.678571</td>\n",
       "      <td>75.446429</td>\n",
       "      <td>55.245536</td>\n",
       "      <td>64.285714</td>\n",
       "      <td>52.678571</td>\n",
       "      <td>0.380222</td>\n",
       "      <td>0.357375</td>\n",
       "      <td>0.355152</td>\n",
       "      <td>1317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>lost_noisy_annotator_p</td>\n",
       "      <td>weighted_loss_xy_SGD_0.0001_1e-05</td>\n",
       "      <td>0</td>\n",
       "      <td>52.678571</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65.178571</td>\n",
       "      <td>89.397321</td>\n",
       "      <td>70.647321</td>\n",
       "      <td>80.357143</td>\n",
       "      <td>71.428571</td>\n",
       "      <td>0.588546</td>\n",
       "      <td>0.533412</td>\n",
       "      <td>0.500174</td>\n",
       "      <td>842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>lost_noisy_annotator_p</td>\n",
       "      <td>weighted_loss_xy_SGD_0.0001_0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>54.464286</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.964286</td>\n",
       "      <td>89.620536</td>\n",
       "      <td>70.982143</td>\n",
       "      <td>81.250000</td>\n",
       "      <td>71.428571</td>\n",
       "      <td>0.599977</td>\n",
       "      <td>0.543434</td>\n",
       "      <td>0.507670</td>\n",
       "      <td>1268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2627</th>\n",
       "      <td>lost</td>\n",
       "      <td>cc_loss_SGD_0.1_0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>69.642857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78.571429</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>91.183036</td>\n",
       "      <td>84.821429</td>\n",
       "      <td>77.678571</td>\n",
       "      <td>0.972744</td>\n",
       "      <td>0.866620</td>\n",
       "      <td>0.841397</td>\n",
       "      <td>1486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2628</th>\n",
       "      <td>lost</td>\n",
       "      <td>weighted_loss_xy_SGD_0.0001_0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>52.678571</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65.178571</td>\n",
       "      <td>89.397321</td>\n",
       "      <td>70.647321</td>\n",
       "      <td>80.357143</td>\n",
       "      <td>71.428571</td>\n",
       "      <td>0.589496</td>\n",
       "      <td>0.534223</td>\n",
       "      <td>0.500778</td>\n",
       "      <td>877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2630</th>\n",
       "      <td>lost</td>\n",
       "      <td>weighted_loss_xy_lstm_SGD_0.0001_0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>51.785714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63.392857</td>\n",
       "      <td>89.285714</td>\n",
       "      <td>68.638393</td>\n",
       "      <td>80.357143</td>\n",
       "      <td>68.750000</td>\n",
       "      <td>0.572772</td>\n",
       "      <td>0.517730</td>\n",
       "      <td>0.486336</td>\n",
       "      <td>1394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2636</th>\n",
       "      <td>lost</td>\n",
       "      <td>weighted_loss_xy_Adam_0.01_0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>30.357143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.714286</td>\n",
       "      <td>95.312500</td>\n",
       "      <td>51.674107</td>\n",
       "      <td>68.750000</td>\n",
       "      <td>34.821429</td>\n",
       "      <td>0.906081</td>\n",
       "      <td>0.774859</td>\n",
       "      <td>0.788931</td>\n",
       "      <td>756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2639</th>\n",
       "      <td>lost</td>\n",
       "      <td>cc_loss_Adam_0.1_1e-05</td>\n",
       "      <td>0</td>\n",
       "      <td>64.285714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.892857</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>83.035714</td>\n",
       "      <td>86.607143</td>\n",
       "      <td>72.321429</td>\n",
       "      <td>0.990323</td>\n",
       "      <td>0.921642</td>\n",
       "      <td>0.916477</td>\n",
       "      <td>1164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>264 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     dataset                               technique fold  \\\n",
       "1     lost_noisy_annotator_p               cc_loss_Adam_0.001_0.0001    0   \n",
       "10    lost_noisy_annotator_p                cc_loss_Adam_0.001_1e-05    0   \n",
       "16    lost_noisy_annotator_p                cc_loss_SGD_0.0001_1e-05    0   \n",
       "17    lost_noisy_annotator_p       weighted_loss_xy_SGD_0.0001_1e-05    0   \n",
       "18    lost_noisy_annotator_p       weighted_loss_xy_SGD_0.0001_0.001    0   \n",
       "...                      ...                                     ...  ...   \n",
       "2627                    lost                   cc_loss_SGD_0.1_0.001    0   \n",
       "2628                    lost      weighted_loss_xy_SGD_0.0001_0.0001    0   \n",
       "2630                    lost  weighted_loss_xy_lstm_SGD_0.0001_0.001    0   \n",
       "2636                    lost        weighted_loss_xy_Adam_0.01_0.001    0   \n",
       "2639                    lost                  cc_loss_Adam_0.1_1e-05    0   \n",
       "\n",
       "      real_test_acc  train_IOU  train_IOU_neg  val_IOU  val_IOU_neg  test_IOU  \\\n",
       "1         71.428571        NaN            NaN      NaN          NaN       NaN   \n",
       "10        67.857143        NaN            NaN      NaN          NaN       NaN   \n",
       "16        40.178571        NaN            NaN      NaN          NaN       NaN   \n",
       "17        52.678571        NaN            NaN      NaN          NaN       NaN   \n",
       "18        54.464286        NaN            NaN      NaN          NaN       NaN   \n",
       "...             ...        ...            ...      ...          ...       ...   \n",
       "2627      69.642857        NaN            NaN      NaN          NaN       NaN   \n",
       "2628      52.678571        NaN            NaN      NaN          NaN       NaN   \n",
       "2630      51.785714        NaN            NaN      NaN          NaN       NaN   \n",
       "2636      30.357143        NaN            NaN      NaN          NaN       NaN   \n",
       "2639      64.285714        NaN            NaN      NaN          NaN       NaN   \n",
       "\n",
       "      test_IOU_neg  surrogate_test_acc  surrogate_train_acc  real_train_acc  \\\n",
       "1              NaN           82.142857           100.000000       87.946429   \n",
       "10             NaN           77.678571           100.000000       87.834821   \n",
       "16             NaN           52.678571            75.446429       55.245536   \n",
       "17             NaN           65.178571            89.397321       70.647321   \n",
       "18             NaN           66.964286            89.620536       70.982143   \n",
       "...            ...                 ...                  ...             ...   \n",
       "2627           NaN           78.571429           100.000000       91.183036   \n",
       "2628           NaN           65.178571            89.397321       70.647321   \n",
       "2630           NaN           63.392857            89.285714       68.638393   \n",
       "2636           NaN           60.714286            95.312500       51.674107   \n",
       "2639           NaN           75.892857           100.000000       83.035714   \n",
       "\n",
       "      surrogate_val_acc  real_val_acc  train_confidence  val_confidence  \\\n",
       "1             86.607143     80.357143          0.996920        0.917440   \n",
       "10            86.607143     82.142857          0.997555        0.928105   \n",
       "16            64.285714     52.678571          0.380222        0.357375   \n",
       "17            80.357143     71.428571          0.588546        0.533412   \n",
       "18            81.250000     71.428571          0.599977        0.543434   \n",
       "...                 ...           ...               ...             ...   \n",
       "2627          84.821429     77.678571          0.972744        0.866620   \n",
       "2628          80.357143     71.428571          0.589496        0.534223   \n",
       "2630          80.357143     68.750000          0.572772        0.517730   \n",
       "2636          68.750000     34.821429          0.906081        0.774859   \n",
       "2639          86.607143     72.321429          0.990323        0.921642   \n",
       "\n",
       "      test_confidence best_epoch  \n",
       "1            0.875374       1271  \n",
       "10           0.908326       1391  \n",
       "16           0.355152       1317  \n",
       "17           0.500174        842  \n",
       "18           0.507670       1268  \n",
       "...               ...        ...  \n",
       "2627         0.841397       1486  \n",
       "2628         0.500778        877  \n",
       "2630         0.486336       1394  \n",
       "2636         0.788931        756  \n",
       "2639         0.916477       1164  \n",
       "\n",
       "[264 rows x 19 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "DataError",
     "evalue": "No numeric types to aggregate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDataError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-4e8d144e9d06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#'weighted_fully_supervised_iexplr',  ]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataset'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'technique'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'fold'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'technique'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'detach'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'technique'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cc_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#a = a.loc[a['fold'].str.contains('0')]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(self, numeric_only)\u001b[0m\n\u001b[1;32m   1497\u001b[0m             \u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m             \u001b[0malt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1499\u001b[0;31m             \u001b[0mnumeric_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1500\u001b[0m         )\n\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36m_cython_agg_general\u001b[0;34m(self, how, alt, numeric_only, min_count)\u001b[0m\n\u001b[1;32m   1014\u001b[0m     ) -> DataFrame:\n\u001b[1;32m   1015\u001b[0m         agg_mgr = self._cython_agg_blocks(\n\u001b[0;32m-> 1016\u001b[0;31m             \u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1017\u001b[0m         )\n\u001b[1;32m   1018\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_agged_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magg_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magg_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36m_cython_agg_blocks\u001b[0;34m(self, how, alt, numeric_only, min_count)\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_mgr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mDataError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No numeric types to aggregate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_mgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDataError\u001b[0m: No numeric types to aggregate"
     ]
    }
   ],
   "source": [
    "#selected_cols = ['fully_supervised',\n",
    "#'cc_loss',\n",
    "#'iexplr_loss',\n",
    "#'weighted_loss_xy',\n",
    "#'weighted_loss_xy_iexplr',\n",
    "##'weighted_loss_y',\n",
    "#'weighted_loss_y_iexplr',\n",
    "#                'weighted_fully_supervised',\n",
    "#'weighted_fully_supervised_iexplr',  ]\n",
    "\n",
    "a = data[columns].groupby(['dataset','technique','fold']).mean().reset_index()\n",
    "a = a.loc[a['technique'].str.contains('detach') | a['technique'].str.contains('cc_loss')]\n",
    "#a = a.loc[a['fold'].str.contains('0')]\n",
    "\n",
    "#a = a.loc[a['technique'].str.contains('fully')]\n",
    "#print(a.technique)\n",
    "#a = (a[a['technique'].isin(selected_cols)])\n",
    "b = a.set_index(['dataset','technique','fold']).unstack(level=0)\n",
    "#b = b[['real_test_acc','surrogate_val_acc','real_val_acc']]\n",
    "b = b.reindex(sorted(b.columns), axis=1)\n",
    "\n",
    "b.to_csv('results_xy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cc_loss_Adam_0.01_1e-06']\n",
      "(10, 20)\n",
      "(0, 19)\n",
      "(10, 19)\n",
      "                      best_epoch                                         ...  \\\n",
      "dataset              MSRCv2_flip                                         ...   \n",
      "fold                           0    1    2  3    4   5    6   7  8    9  ...   \n",
      "tech                                                                     ...   \n",
      "cc                           762  709  771  9  792   6   16  20  8  798  ...   \n",
      "lstm_freeze+pretrain         209  768    1  1    1  54  972   1  1   67  ...   \n",
      "\n",
      "                     val_confidence                                          \\\n",
      "dataset                 MSRCv2_flip                                           \n",
      "fold                              0         1         2         3         4   \n",
      "tech                                                                          \n",
      "cc                         0.899774  0.915350  0.927271  0.722689  0.908199   \n",
      "lstm_freeze+pretrain       0.907027  0.911633  0.927271  0.722689  0.908199   \n",
      "\n",
      "                                                                        \n",
      "dataset                                                                 \n",
      "fold                         5         6         7         8         9  \n",
      "tech                                                                    \n",
      "cc                    0.691592  0.797737  0.819196  0.758607  0.910242  \n",
      "lstm_freeze+pretrain  0.718990  0.921884  0.819196  0.758607  0.837290  \n",
      "\n",
      "[2 rows x 170 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "c = data[columns]\n",
    "\n",
    "c = c.loc[c['technique'].str.contains('pretrain') | c['technique'].str.contains('cc_loss') | c['technique'].str.contains('freeze') | c['technique'].str.contains('pq') ]\n",
    "c = c.loc[c['dataset'].str.endswith('MSRCv2_flip')]\n",
    "c = c.loc[~c['technique'].str.contains('_1_')]\n",
    "\n",
    "cc = c.loc[c['technique'].str.contains('cc_loss')]\n",
    "cc = cc.loc[~cc['technique'].str.contains('1e-05')]\n",
    "cc = cc.loc[~cc['technique'].str.contains('1_0.001_')]\n",
    "\n",
    "cc['tech'] = 'cc'\n",
    "print(cc['technique'].unique())\n",
    "print(cc.shape)\n",
    "cc = cc.loc[cc.groupby(['dataset','tech','fold'])['surrogate_val_acc'].idxmax()]\n",
    "\n",
    "\n",
    "lstm = c.loc[c['technique'].str.contains('weighted_loss_xy_lstm_iexplr')]\n",
    "#print(lstm)\n",
    "lstm = lstm.loc[~lstm['technique'].str.contains('fully')]\n",
    "\n",
    "lstm_pq = lstm.loc[lstm['technique'].str.endswith('_100_pq')]\n",
    "lstm_freeze = lstm.loc[lstm['technique'].str.contains('_100_freeze')]\n",
    "lstm_pretrain = lstm.loc[lstm['technique'].str.contains('_100_pretrain')]\n",
    "print(lstm_freeze.shape)\n",
    "print(lstm_pretrain.shape)\n",
    "lstm_pq['tech'] = 'lstm'\n",
    "lstm_freeze['tech'] = 'lstm_freeze'\n",
    "lstm_pretrain['tech'] = 'lstm_freeze+pretrain'\n",
    "\n",
    "lstm_pq = lstm_pq.loc[lstm_pq.groupby(['dataset','tech','fold'])['surrogate_val_acc'].idxmax()]\n",
    "lstm_freeze = lstm_freeze.loc[lstm_freeze.groupby(['dataset','tech','fold'])['surrogate_val_acc'].idxmax()]\n",
    "lstm_pretrain = lstm_pretrain.loc[lstm_pretrain.groupby(['dataset','tech','fold'])['surrogate_val_acc'].idxmax()]\n",
    "\n",
    "c = pd.concat([cc, lstm_pq, lstm_freeze, lstm_pretrain])\n",
    "\n",
    "#c = c[['real_test_acc','surrogate_val_acc','real_val_acc']]\n",
    "c = c.reindex(sorted(c.columns), axis=1)\n",
    "#c = c[c['dataset'] == 'MSRCv2_lstm']\n",
    "c = c.pivot(index=\"tech\", columns=[\"dataset\",\"fold\"])\n",
    "print(c.head())\n",
    "#c = c.set_index(['dataset','tech','fold']).unstack(level=0)\n",
    "\n",
    "c.to_csv('results_xy_lstm_val_surr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   dataset                                          technique fold  \\\n",
      "0   MSRCv2  weighted_loss_xy_lstm_iexplr_Adam_0.1_0.0001_1...    1   \n",
      "1   MSRCv2  weighted_loss_xy_lstm_iexplr_Adam_0.1_0.0001_1...    3   \n",
      "2   MSRCv2  weighted_loss_xy_lstm_iexplr_Adam_0.1_0.0001_1...    9   \n",
      "3   MSRCv2  weighted_loss_xy_lstm_iexplr_Adam_0.001_0.0001...    4   \n",
      "4   MSRCv2  weighted_loss_xy_lstm_iexplr_Adam_0.1_0.0001_1...    1   \n",
      "..     ...                                                ...  ...   \n",
      "80  MSRCv2  weighted_loss_xy_lstm_iexplr_Adam_0.01_0.0001_...    0   \n",
      "81  MSRCv2  weighted_loss_xy_lstm_iexplr_Adam_0.01_0.0001_...    2   \n",
      "82  MSRCv2  weighted_loss_xy_lstm_iexplr_Adam_0.01_0.0001_...    4   \n",
      "83  MSRCv2  weighted_loss_xy_lstm_iexplr_Adam_0.01_0.0001_...    6   \n",
      "84  MSRCv2  weighted_loss_xy_lstm_iexplr_Adam_0.01_0.0001_...    8   \n",
      "\n",
      "    real_test_acc  train_IOU  train_IOU_neg   val_IOU  val_IOU_neg  test_IOU  \\\n",
      "0       42.857143  -1.000000      -1.000000 -1.000000    -1.000000 -1.000000   \n",
      "1       49.142857  -1.000000      -1.000000 -1.000000    -1.000000 -1.000000   \n",
      "2       46.857143  -1.000000      -1.000000 -1.000000    -1.000000 -1.000000   \n",
      "3       50.857143   0.778138       0.389749  0.602322     0.305355  0.593703   \n",
      "4       42.857143  -1.000000      -1.000000 -1.000000    -1.000000 -1.000000   \n",
      "..            ...        ...            ...       ...          ...       ...   \n",
      "80      46.857143   0.618168       0.319188  0.574281     0.286061  0.497912   \n",
      "81      54.285714   0.678016       0.323420  0.527358     0.251766  0.526374   \n",
      "82      51.428571   0.697017       0.323409  0.565075     0.257470  0.521413   \n",
      "83      39.428571   0.757151       0.345889  0.455576     0.292497  0.469170   \n",
      "84      56.000000   0.773270       0.330598  0.489417     0.297059  0.476107   \n",
      "\n",
      "    test_IOU_neg  surrogate_test_acc  surrogate_train_acc  real_train_acc  \\\n",
      "0      -1.000000           66.857143            92.571429       59.214286   \n",
      "1      -1.000000           72.000000            92.500000       60.285714   \n",
      "2      -1.000000           68.000000            92.500000       58.285714   \n",
      "3       0.304966           68.000000            89.142857       61.142857   \n",
      "4      -1.000000           69.142857            82.785714       52.714286   \n",
      "..           ...                 ...                  ...             ...   \n",
      "80      0.310386           67.428571            91.500000       61.000000   \n",
      "81      0.272855           74.285714            91.357143       61.142857   \n",
      "82      0.275621           70.285714            86.214286       59.785714   \n",
      "83      0.293641           70.285714            91.357143       60.500000   \n",
      "84      0.286703           76.000000            85.714286       60.071429   \n",
      "\n",
      "    surrogate_val_acc  real_val_acc  train_confidence  val_confidence  \\\n",
      "0           75.428571     56.000000          0.920792        0.906958   \n",
      "1           80.000000     51.428571          0.901910        0.863678   \n",
      "2           81.142857     51.428571          0.919587        0.891553   \n",
      "3           70.857143     57.142857          0.816671        0.785752   \n",
      "4           75.428571     51.428571          0.835475        0.834003   \n",
      "..                ...           ...               ...             ...   \n",
      "80          70.285714     48.571429          0.905880        0.861190   \n",
      "81          78.285714     58.285714          0.902887        0.898112   \n",
      "82          76.000000     54.857143          0.886366        0.888712   \n",
      "83          71.428571     50.857143          0.914794        0.881649   \n",
      "84          76.571429     57.142857          0.884908        0.895020   \n",
      "\n",
      "    test_confidence best_epoch  \n",
      "0          0.885986        864  \n",
      "1          0.861157          1  \n",
      "2          0.905744          1  \n",
      "3          0.761706         55  \n",
      "4          0.812259          1  \n",
      "..              ...        ...  \n",
      "80         0.848924        994  \n",
      "81         0.901811         30  \n",
      "82         0.880126         30  \n",
      "83         0.873266        541  \n",
      "84         0.866666         99  \n",
      "\n",
      "[85 rows x 19 columns]\n",
      "                     best_epoch                                          ...  \\\n",
      "dataset                  MSRCv2                                          ...   \n",
      "fold                          0   1   2   3    4    5    6    7   8   9  ...   \n",
      "tech                                                                     ...   \n",
      "lstm                          9  18  30  36   30   14  541   26  99   9  ...   \n",
      "lstm_freeze                 970  71  54  72  128  100  737    1  97  91  ...   \n",
      "lstm_freeze+pretrain         52  81  57   1   55  205   72  810  84  63  ...   \n",
      "\n",
      "                     val_confidence                                          \\\n",
      "dataset                      MSRCv2                                           \n",
      "fold                              0         1         2         3         4   \n",
      "tech                                                                          \n",
      "lstm                       0.865252  0.855892  0.898112  0.870795  0.888712   \n",
      "lstm_freeze                0.929237  0.872472  0.844321  0.872246  0.883464   \n",
      "lstm_freeze+pretrain       0.838747  0.861769  0.873593  0.858241  0.785752   \n",
      "\n",
      "                                                                        \n",
      "dataset                                                                 \n",
      "fold                         5         6         7         8         9  \n",
      "tech                                                                    \n",
      "lstm                  0.809306  0.881649  0.883214  0.895020  0.763887  \n",
      "lstm_freeze           0.812683  0.872473  0.849939  0.915389  0.868687  \n",
      "lstm_freeze+pretrain  0.874018  0.888503  0.889769  0.903074  0.808879  \n",
      "\n",
      "[3 rows x 170 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yatin/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/yatin/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/yatin/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.DataFrame(columns=columns) \n",
    "count = 0\n",
    "for folder in res:\n",
    "    \n",
    "    logfile = os.path.join(folder, \"logs\",\"log.json_lstm\")\n",
    "    if(not(os.path.exists(logfile))):\n",
    "        continue\n",
    "    df = pd.read_json(logfile, lines=True)\n",
    "    df = df[df[\"epoch\"] == -2]\n",
    "    #print(df.columns.values)\n",
    "    names = folder.split(\"/\")\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        if(row[\"epoch\"] == -2):\n",
    "            data.loc[count] = row\n",
    "            data.loc[count,\"dataset\"] = names[-4]\n",
    "            #data.loc[count,\"model\"] = names[-3]\n",
    "            data.loc[count,\"technique\"] = names[-2]\n",
    "            data.loc[count,\"fold\"] = names[-1]\n",
    "            count+=1\n",
    "\n",
    "\n",
    "c = data[columns]\n",
    "print(data)\n",
    "c = c.loc[c['technique'].str.contains('pretrain') | c['technique'].str.contains('cc_loss') | c['technique'].str.contains('freeze') | c['technique'].str.contains('pq') ]\n",
    "c = c.loc[c['dataset'].str.endswith('MSRCv2')]\n",
    "c = c.loc[~c['technique'].str.contains('_1_')]\n",
    "\n",
    "cc = c.loc[c['technique'].str.contains('cc_loss')]\n",
    "cc = cc.loc[~cc['technique'].str.contains('1e-05')]\n",
    "cc = cc.loc[~cc['technique'].str.contains('1_0.001_')]\n",
    "\n",
    "cc['tech'] = 'cc'\n",
    "cc = cc.loc[cc.groupby(['dataset','tech','fold'])['real_val_acc'].idxmax()]\n",
    "\n",
    "\n",
    "lstm = c.loc[c['technique'].str.contains('weighted_loss_xy_lstm_iexplr')]\n",
    "#print(lstm)\n",
    "lstm = lstm.loc[~lstm['technique'].str.contains('fully')]\n",
    "\n",
    "lstm_pq = lstm.loc[lstm['technique'].str.endswith('_100_pq')]\n",
    "lstm_freeze = lstm.loc[lstm['technique'].str.contains('_100_freeze')]\n",
    "lstm_pretrain = lstm.loc[lstm['technique'].str.contains('_100_pretrain')]\n",
    "\n",
    "lstm_pq['tech'] = 'lstm'\n",
    "lstm_freeze['tech'] = 'lstm_freeze'\n",
    "lstm_pretrain['tech'] = 'lstm_freeze+pretrain'\n",
    "\n",
    "lstm_pq = lstm_pq.loc[lstm_pq.groupby(['dataset','tech','fold'])['real_val_acc'].idxmax()]\n",
    "lstm_freeze = lstm_freeze.loc[lstm_freeze.groupby(['dataset','tech','fold'])['real_val_acc'].idxmax()]\n",
    "lstm_pretrain = lstm_pretrain.loc[lstm_pretrain.groupby(['dataset','tech','fold'])['real_val_acc'].idxmax()]\n",
    "\n",
    "c = pd.concat([cc, lstm_pq, lstm_freeze, lstm_pretrain])\n",
    "\n",
    "c = c.reindex(sorted(c.columns), axis=1)\n",
    "#c = c[c['dataset'] == 'MSRCv2_lstm']\n",
    "c = c.pivot(index=\"tech\", columns=[\"dataset\",\"fold\"])\n",
    "print(c.head())\n",
    "#c = c.set_index(['dataset','tech','fold']).unstack(level=0)\n",
    "\n",
    "c.to_csv('results_xy_lstm_val_comb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
