{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import scipy.io\n",
    "from dataset import Dataset, loadTrain\n",
    "from losses import  cc_loss, weighted_cc_loss, min_loss, naive_loss, iexplr_loss, regularized_cc_loss, sample_loss_function, sample_reward_function, select_loss_function, select_reward_function, svm_loss, cour_loss\n",
    "from networks import LSTM\n",
    "from networks import Prediction_Net,LeNet5, Prediction_Net_Linear, Selection_Net, Phi_Net, G_Net_Tie, G_Net_Full, G_Net_Hyperparameter, G_Net_Y, G_Net_XY\n",
    "import sys\n",
    "from IPython.core.debugger import Pdb\n",
    "import random\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.001\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "epsilon = 10e-12\n",
    "\n",
    "#Reproducibility\n",
    "def set_random_seeds(random_seed):\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "set_random_seeds(1)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "def weighted_train(epoch, train_loader, p_net, p_optimizer, g_net, g_optimizer, method, class_dim):\n",
    "    p_net.train()\n",
    "    \n",
    "    row = np.asarray(list(range(class_dim)))\n",
    "    one_hot_gpu = torch.zeros((row.size, class_dim))\n",
    "    one_hot_gpu = one_hot_gpu.to(device)\n",
    "    one_hot_gpu[torch.arange(row.size), row] = 1\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        p_optimizer.zero_grad()\n",
    "        output = p_net(data)\n",
    "        batch = data.shape[0]\n",
    "        \n",
    "        one_hot = one_hot_gpu\n",
    "        one_hot = one_hot.expand(batch, class_dim, class_dim).reshape(batch*class_dim, class_dim)\n",
    "        #Pdb().set_trace()\n",
    "        \n",
    "        #class_dim is EOS\n",
    "        #class_dim +1 is NULL\n",
    "        \n",
    "        if(\"loss_xy_lstm\" in method):\n",
    "            \n",
    "            max_req = int(torch.max(target.sum(dim=1)).item())\n",
    "            relevant_indices = target.nonzero()\n",
    "            \n",
    "            input_x = data[relevant_indices[:,0]]\n",
    "            input_y = relevant_indices[:,1]\n",
    "            input_y_onehot = torch.zeros((input_y.shape[0], class_dim+2))\n",
    "            input_y_onehot = input_y_onehot.to(device)\n",
    "            \n",
    "            input_y = torch.unsqueeze(input_y, dim = -1)\n",
    "            input_y_onehot.scatter_(1, input_y, 1)\n",
    "            input_y = input_y_onehot\n",
    "            \n",
    "            \n",
    "            input_x = torch.unsqueeze(input_x, dim = 0)\n",
    "            input_x = torch.repeat_interleave(input_x, max_req+1, dim=0)\n",
    "            \n",
    "            input_y = torch.unsqueeze(input_y, dim = 0)\n",
    "            input_y = torch.repeat_interleave(input_y, max_req+1, dim=0)\n",
    "            \n",
    "            target_set = target[relevant_indices[:,0]]\n",
    "            \n",
    "            target_set2 = torch.ones(target_set.shape[0], max_req)\n",
    "            target_set2 = target_set2.to(device)\n",
    "            target_set = torch.cat([target_set, target_set2], dim=1)\n",
    "            \n",
    "            idx = torch.arange(target_set.shape[1], 0, -1).to(device).float()\n",
    "            tmp2 = target_set * idx\n",
    "            indices = torch.topk(tmp2, k = max_req+1, dim=1)[1]\n",
    "            #mask = (indices > output_dim)\n",
    "            indices[indices > class_dim] = class_dim+1\n",
    "            indices = torch.transpose(indices, dim0=0, dim1=1)\n",
    "            indices = torch.unsqueeze(indices, dim = -1)\n",
    "            \n",
    "            target_set = torch.zeros(indices.shape[0], indices.shape[1], class_dim+2)\n",
    "            target_set = target_set.to(device)\n",
    "            target_set = target_set.to(device)\n",
    "            target_set = target_set.scatter(-1, indices, 1)\n",
    "            \n",
    "            #Pdb().set_trace()\n",
    "            g_output = torch.zeros(input_x.shape[1], 1)\n",
    "            g_output = g_output.to(device)\n",
    "            \n",
    "            g_net.hidden_cell = (torch.zeros(1, input_x.shape[1], g_net.hidden_layer_size).to(device),\n",
    "                                torch.zeros(1, input_x.shape[1] , g_net.hidden_layer_size).to(device))\n",
    "            \n",
    "            \n",
    "            #Pdb().set_trace()\n",
    "            for seq_step in range(max_req+1):\n",
    "                x = input_x[seq_step]\n",
    "                y = input_y[seq_step]\n",
    "                s = target_set[seq_step]\n",
    "                \n",
    "                \n",
    "                    \n",
    "                g_optimizer.zero_grad()\n",
    "                \n",
    "                \n",
    "                if(seq_step == 0):\n",
    "                    y_pred = g_net(x.clone(), y.clone(), y.clone())\n",
    "                else:\n",
    "                    y_pred = g_net(x.clone(), y.clone(), target_set[seq_step-1].clone())\n",
    "                \n",
    "                \n",
    "                #index = s.argmax(dim=1).unsqueeze(-1)\n",
    "                y_pred = y_pred.log_softmax(dim=1)\n",
    "                \n",
    "                \n",
    "                loss = nn.CrossEntropyLoss(reduce = False, ignore_index = class_dim+1)\n",
    "                \n",
    "                g_output -= (loss(y_pred, s.argmax(dim=1))).unsqueeze(-1)\n",
    "                #g_output += (torch.gather(y_pred, 1, index))\n",
    "                #if(epoch == 200):\n",
    "                #    Pdb().set_trace()\n",
    "                \n",
    "            g_output = g_output.flatten()\n",
    "            \n",
    "            temp = torch.zeros((batch*class_dim))\n",
    "            temp = temp.to(device)\n",
    "            project_index = relevant_indices[:,0] * class_dim + relevant_indices[:,1]\n",
    "            temp[project_index] = g_output\n",
    "            g_output = temp\n",
    "            \n",
    "            \n",
    "        elif(\"loss_xy\" in method):\n",
    "            \n",
    "            relevant_indices = target.nonzero()\n",
    "            input_x = data[relevant_indices[:,0]]\n",
    "            input_y = relevant_indices[:,1]\n",
    "            \n",
    "            g_output = g_net((input_x,input_y),device)\n",
    "            log_sigmoid = nn.LogSigmoid()\n",
    "            \n",
    "            target_concat = target[relevant_indices[:,0]]\n",
    "            g_output = log_sigmoid(g_output) * target_concat + (log_sigmoid(-g_output))*(1-target_concat)\n",
    "            g_output = g_output.sum(dim=1)\n",
    "            \n",
    "            temp = torch.zeros((batch*class_dim))\n",
    "            temp = temp.to(device)\n",
    "            project_index = relevant_indices[:,0] * class_dim + relevant_indices[:,1]\n",
    "            temp[project_index] = g_output\n",
    "            g_output = temp\n",
    "        else:    \n",
    "            g_output = g_net(one_hot)\n",
    "            \n",
    "            log_sigmoid = nn.LogSigmoid()\n",
    "            target_concat = target.repeat_interleave(class_dim, dim=0)\n",
    "            g_output = log_sigmoid(g_output) * target_concat + (log_sigmoid(-g_output))*(1-target_concat)\n",
    "            \n",
    "            g_output = g_output.sum(dim=1)\n",
    "        \n",
    "        split_g_output = g_output.view(batch, class_dim)\n",
    "        \n",
    "        if('iexplr' in method):\n",
    "            log_prob =  split_g_output + torch.log_softmax(output, dim=1)\n",
    "            prob = torch.exp(log_prob).detach()\n",
    "            #prob = log_prob.detach()\n",
    "            target_probs = (prob*target.float()).sum(dim=1)\n",
    "            mask = ((target == 1) & (abs(prob) > epsilon))\n",
    "            #Pdb().set_trace()\n",
    "            loss = -(prob[mask]*log_prob[mask]/ target_probs.unsqueeze(1).expand_as(mask)[mask]).sum() / mask.size(0)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            mask = (target != 1)\n",
    "            \n",
    "            log_target_prob2 = split_g_output\n",
    "            log_target_prob_for_max2 = (split_g_output).masked_fill(mask,-float('inf'))\n",
    "            log_max_prob2,max_prob_index2 = log_target_prob_for_max2.max(dim=1)\n",
    "            exp_argument2 = log_target_prob2 - log_max_prob2.unsqueeze(dim=1)\n",
    "            \n",
    "            norm = log_max_prob2 +torch.log((target*torch.exp(exp_argument2*target)).sum(dim=1))\n",
    "            #print(norm)\n",
    "            #norm = torch.ones_like(norm)\n",
    "            #norm = norm * np.log(pow(0.2,23))\n",
    "            #log_target_prob = split_g_output  +  F.log_softmax(output, dim = 1)\n",
    "            #log_target_prob_for_max = (split_g_output  +  F.log_softmax(output, dim = 1)).masked_fill(mask,-float('inf'))\n",
    "            \n",
    "            log_target_prob = split_g_output  - norm.unsqueeze(dim=1)  +  F.log_softmax(output, dim = 1)\n",
    "            log_target_prob_for_max = (split_g_output - norm.unsqueeze(dim=1) +  F.log_softmax(output, dim = 1)).masked_fill(mask,-float('inf'))\n",
    "            \n",
    "            #log_target_prob = split_g_output  +  F.log_softmax(output, dim = 1)\n",
    "            #log_target_prob_for_max = (split_g_output +  F.log_softmax(output, dim = 1)).masked_fill(mask,-float('inf'))\n",
    "            \n",
    "            log_max_prob,max_prob_index = log_target_prob_for_max.max(dim=1)\n",
    "            exp_argument = log_target_prob - log_max_prob.unsqueeze(dim=1) \n",
    "            #exp_argument = exp_argument\n",
    "            \n",
    "            #print(torch.exp(norm))\n",
    "            \n",
    "            summ = (target*torch.exp(exp_argument*target)).sum(dim=1)\n",
    "            #summ = summ/norm\n",
    "            log_total_prob = log_max_prob + torch.log(summ + epsilon)\n",
    "            loss = (-1.0*log_total_prob).mean(dim=-1)\n",
    "            #if(torch.isnan(loss)):\n",
    "            #    Pdb().set_trace()\n",
    "        loss.backward()\n",
    "        \n",
    "        p_optimizer.step()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "          print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "         \n",
    "def p_accuracy(test_data, p_net, loss_function):\n",
    "    p_net.eval()\n",
    "    correct = 0\n",
    "    loss = 0\n",
    "    confidence = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_data:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = p_net.forward(data)\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            \n",
    "            prob = torch.softmax(output, dim=1)\n",
    "            confidence += torch.gather(prob, 1, pred).sum()\n",
    "            \n",
    "            correct += torch.gather(target, 1, pred).sum()\n",
    "            this_loss =  (loss_function(output, target))\n",
    "            loss += this_loss * len(data)\n",
    "         \n",
    "    return {'acc':(100. * float(correct.item()) / len(test_data.dataset)), 'loss':loss.item()/(len(test_data.dataset)), 'confidence': confidence.item()/len(test_data.dataset)}\n",
    "\n",
    "def intersection(s1, s2):\n",
    "    count = 0\n",
    "    for i in s1:\n",
    "        for j in s2:\n",
    "            if(i == j):\n",
    "                count+=1\n",
    "    return count\n",
    "\n",
    "def union(s1, s2):\n",
    "    return len(s1) + len(s2) - intersection(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_accuracy(epoch, real_train_loader, train_loader, g_net, technique, class_dim):\n",
    "    g_net.eval()\n",
    "    \n",
    "    row = np.asarray(list(range(class_dim)))\n",
    "    one_hot_gpu = torch.zeros((row.size, class_dim))\n",
    "    one_hot_gpu = one_hot_gpu.to(device)\n",
    "    one_hot_gpu[torch.arange(row.size), row] = 1\n",
    "    \n",
    "    correct = []\n",
    "    predicted = []\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        temp = target.nonzero()[:,1].tolist()\n",
    "        correct.append(temp)\n",
    "    sum_iou_neg = 0 \n",
    "    remove = 0\n",
    "    for batch_idx, (data, target) in enumerate(real_train_loader):\n",
    "        input_x = data\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        #output = p_net(data)\n",
    "        batch = data.shape[0]\n",
    "        \n",
    "            \n",
    "        #Pdb().set_trace()\n",
    "        g_net.hidden_cell = (torch.zeros(1, input_x.shape[0], g_net.hidden_layer_size).to(device),\n",
    "                            torch.zeros(1, input_x.shape[0] , g_net.hidden_layer_size).to(device))\n",
    "        \n",
    "        \n",
    "        y = target\n",
    "        \n",
    "        count = 0\n",
    "        run_sum = 0\n",
    "        \n",
    "        for gold_y_prime in correct[batch_idx]:\n",
    "            if(gold_y_prime == target.argmax()):\n",
    "                continue\n",
    "            gold_y = torch.zeros_like(target).to(device)\n",
    "            gold_y[0,gold_y_prime] = 1\n",
    "            y = gold_y\n",
    "            \n",
    "            row = torch.tensor(list(range(class_dim+1))).to(device)\n",
    "            mask = (row > -1).unsqueeze(0).to(device)\n",
    "            mask = mask.float()\n",
    "            pred_set = []\n",
    "            while True:\n",
    "                x = data\n",
    "\n",
    "                y_pred = g_net(x.clone(), gold_y.clone(), y.clone())\n",
    "\n",
    "                y_pred = y_pred.softmax(dim=1) * mask\n",
    "                \n",
    "                if(torch.isnan(y_pred.sum())):\n",
    "                    return (0,0)\n",
    "                \n",
    "                y = y_pred.argmax(dim=1)\n",
    "                y_pred[y_pred!=0] = 0\n",
    "                y_pred[0, y] = 1\n",
    "\n",
    "                \n",
    "                \n",
    "                if(y.item() == class_dim):\n",
    "                    break\n",
    "                mask = (row > y).unsqueeze(0).to(device)\n",
    "                mask = mask.float()\n",
    "                pred_set.append(y.item())\n",
    "                y = y_pred\n",
    "            inter = intersection(correct[batch_idx], pred_set)\n",
    "            unio = union(correct[batch_idx], pred_set)\n",
    "            count += 1\n",
    "            run_sum += float(inter)/unio\n",
    "        if(count == 0):\n",
    "            remove+=1\n",
    "        else:\n",
    "            sum_iou_neg += run_sum/count\n",
    "        \n",
    "    for batch_idx, (data, target) in enumerate(real_train_loader):\n",
    "        input_x = data\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        #output = p_net(data)\n",
    "        batch = data.shape[0]\n",
    "        \n",
    "            \n",
    "        #Pdb().set_trace()\n",
    "        g_net.hidden_cell = (torch.zeros(1, input_x.shape[0], g_net.hidden_layer_size).to(device),\n",
    "                            torch.zeros(1, input_x.shape[0] , g_net.hidden_layer_size).to(device))\n",
    "        \n",
    "        #y = target.argmax(dim=1)\n",
    "        #y = y.unsqueeze(-1)\n",
    "        #Pdb().set_trace()\n",
    "        y = target\n",
    "        gold_y = target\n",
    "        #Pdb().set_trace()\n",
    "        #print(y.shape)\n",
    "        row = torch.tensor(list(range(class_dim+1))).to(device)\n",
    "        mask = (row > -1).unsqueeze(0).to(device)\n",
    "        mask = mask.float()\n",
    "        pred_set = []\n",
    "        while True:\n",
    "            x = data\n",
    "            \n",
    "            y_pred = g_net(x.clone(), gold_y.clone(), y.clone())\n",
    "            \n",
    "            y_pred = y_pred.softmax(dim=1) * mask\n",
    "            y = y_pred.argmax(dim=1)\n",
    "            y_pred[y_pred!=0] = 0\n",
    "            y_pred[0, y] = 1\n",
    "            \n",
    "            if(torch.isnan(y_pred.sum())):\n",
    "                return (0,0)\n",
    "                \n",
    "            if(y.item() == class_dim):\n",
    "                break\n",
    "            mask = (row > y).unsqueeze(0).to(device)\n",
    "            mask = mask.float()\n",
    "            pred_set.append(y.item())\n",
    "            y = y_pred\n",
    "        predicted.append(pred_set)\n",
    "    \n",
    "            #predicted.append(pred_set)\n",
    "    \n",
    "    inter = 0\n",
    "    unio = 0\n",
    "    count = 0\n",
    "    sum_iou = 0\n",
    "    for i in range(len(correct)):\n",
    "        inter = intersection(correct[i], predicted[i])\n",
    "        unio = union(correct[i], predicted[i])\n",
    "        sum_iou += float(inter)/unio\n",
    "        count+=1\n",
    "    \n",
    "    return(float(sum_iou)/count , float(sum_iou_neg)/(count-remove))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../networks.py:32: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(self.fc1.weight)\n",
      "../networks.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(self.fc2.weight)\n",
      "../networks.py:34: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(self.fc3.weight)\n",
      "/home/yatin/anaconda3/lib/python3.7/site-packages/torch/serialization.py:454: SourceChangeWarning: source code of class 'torch.nn.modules.rnn.LSTM' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/yatin/anaconda3/lib/python3.7/site-packages/torch/serialization.py:454: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/yatin/anaconda3/lib/python3.7/site-packages/torch/serialization.py:454: SourceChangeWarning: source code of class 'torch.nn.modules.sparse.Embedding' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSRCv2 4 train_best.pth weighted_loss_xy_lstm_iexplr_Adam_0.01_0.0001_100_freeze\n",
      "(0.8004801587301584, 0.36206921986516954)\n",
      "(0.5595714285714289, 0.2764754357775831)\n",
      "(0.5536099773242631, 0.2906220963995356)\n"
     ]
    }
   ],
   "source": [
    "dump_dir = \"../results/lstm\"\n",
    "dataset_folder = \"../datasets\"\n",
    "datasets = \"MSRCv2\"\n",
    "#technique = \"weighted_loss_xy_lstm_SGD_0.1_1e-06_100\"\n",
    "datasets = [str(item) for item in datasets.split(',')]\n",
    "model = '3layer'\n",
    "k = 10\n",
    "\n",
    "\n",
    "for filename in datasets:\n",
    "    \n",
    "    #tar = pd.read_csv(\"../file.csv\")\n",
    "    #for rowIndex, row in tar.iterrows(): #iterate over rows\n",
    "        \n",
    "    #    for columnIndex, value in row.items():\n",
    "        directory = \"weighted_loss_xy_lstm_iexplr_Adam_0.01_0.0001_100_freeze\"\n",
    "    #        if(rowIndex >= 4):\n",
    "        model = \"train_best.pth\"\n",
    "    #        else:\n",
    "    #            model = \"train_best.pth\"\n",
    "        fold_no = int(float(4))\n",
    "\n",
    "\n",
    "        train_dataset, real_train_dataset, val_dataset, real_val_dataset, test_dataset, real_test_dataset, input_dim, output_dim = loadTrain(os.path.join(dataset_folder,filename+\".mat\"), fold_no, k)\n",
    "\n",
    "        batch_size_test = 1\n",
    "        batch_size_train = 1\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "          batch_size=batch_size_train, shuffle=False)\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "          batch_size=batch_size_test, shuffle=False)\n",
    "        val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "          batch_size=batch_size_test, shuffle=False)\n",
    "\n",
    "        real_train_loader = torch.utils.data.DataLoader(real_train_dataset,\n",
    "          batch_size=batch_size_train, shuffle=False)\n",
    "        real_test_loader = torch.utils.data.DataLoader(real_test_dataset,\n",
    "          batch_size=batch_size_test, shuffle=False)\n",
    "        real_val_loader = torch.utils.data.DataLoader(real_val_dataset,\n",
    "          batch_size=batch_size_test, shuffle=False)\n",
    "\n",
    "\n",
    "        directories = os.listdir(os.path.join(dump_dir, filename, '3layer'))\n",
    "    #directories = [\"weighted_loss_xy_lstm_SGD_0.1_1e-05_100\"]\n",
    "    #for directory in directories:\n",
    "        if(not('lstm' in directory)):\n",
    "            continue\n",
    "        #if(not('pq' in directory) and not('freeze' in directory)):\n",
    "        #    continue\n",
    "\n",
    "        logs = []\n",
    "        flag = False\n",
    "\n",
    "\n",
    "\n",
    "        dataset_technique_path = os.path.join(filename, '3layer', directory, str(fold_no))\n",
    "\n",
    "        p_net = Prediction_Net(input_dim, output_dim)\n",
    "        p_net.to(device) \n",
    "\n",
    "\n",
    "        g_net = LSTM(input_dim, output_dim, directory)\n",
    "\n",
    "\n",
    "        g_net.to(device)\n",
    "        if(not(os.path.exists(os.path.join(dump_dir, filename, '3layer', directory, str(fold_no), \"models\")))):\n",
    "            continue\n",
    "        models = os.listdir(os.path.join(dump_dir, filename, '3layer', directory, str(fold_no), \"models\"))\n",
    "        models = ['train_best.pth', 'train_best_real.pth']\n",
    "        result_log_filename_json = os.path.join(dump_dir, dataset_technique_path, \"logs\", \"log.json\")\n",
    "        if(not(os.path.exists(result_log_filename_json))):\n",
    "            continue\n",
    "        df = pd.read_json(result_log_filename_json, orient ='records', lines = True)\n",
    "\n",
    "        tr_accuracies = [-1] * df.shape[0]\n",
    "        val_accuracies = [-1] * df.shape[0]\n",
    "        test_accuracies = [-1] * df.shape[0]\n",
    "\n",
    "        tr_accuracies_n = [-1] * df.shape[0]\n",
    "        val_accuracies_n = [-1] * df.shape[0]\n",
    "        test_accuracies_n = [-1] * df.shape[0]\n",
    "\n",
    "        #for model in models:\n",
    "        print(filename+\" \"+str(fold_no)+\" \"+model +\" \"+ directory)\n",
    "        if(not('pth') in model):\n",
    "            continue\n",
    "        train_checkpoint = os.path.join(dump_dir, filename, '3layer', directory, str(fold_no), \"models\", model) \n",
    "        checkpoint = torch.load(train_checkpoint)\n",
    "\n",
    "        p_net.load_state_dict(checkpoint['p_net_state_dict'])\n",
    "        g_net.load_state_dict(checkpoint['g_net'].state_dict())\n",
    "\n",
    "\n",
    "\n",
    "        best_val_epoch = -1\n",
    "\n",
    "        acc = lstm_accuracy(-1, real_train_loader, train_loader, g_net, directory, output_dim)\n",
    "        print(acc)\n",
    "        if(acc[0] == 0):\n",
    "            print(\"NAN\")\n",
    "        if(\"real\" in model):\n",
    "            tr_accuracies[-1] = acc[0]\n",
    "            tr_accuracies_n[-1] = acc[1]\n",
    "        else:\n",
    "            tr_accuracies[-2] = acc[0]\n",
    "            tr_accuracies_n[-2] = acc[1]\n",
    "\n",
    "        acc = lstm_accuracy(-1, real_val_loader, val_loader, g_net, directory, output_dim)\n",
    "        print(acc)\n",
    "        epoch = model.split(\"_\")[1].split(\".\")[0]\n",
    "        if(\"real\" in model):\n",
    "            val_accuracies[-1] = acc[0]\n",
    "            val_accuracies_n[-1] = acc[1]\n",
    "        else:\n",
    "            val_accuracies[-2] = acc[0]\n",
    "            val_accuracies_n[-2] = acc[1]\n",
    "\n",
    "        acc = lstm_accuracy(-1, real_test_loader, test_loader, g_net, directory, output_dim)\n",
    "        print(acc)\n",
    "        epoch = model.split(\"_\")[1].split(\".\")[0]\n",
    "        if(\"real\" in model):\n",
    "            test_accuracies[-1] = acc[0]\n",
    "            test_accuracies_n[-1] = acc[1]\n",
    "        else:\n",
    "            test_accuracies[-2] = acc[0]\n",
    "            test_accuracies_n[-2] = acc[1]\n",
    "\n",
    "\n",
    "\n",
    "        df['train_IOU']  = tr_accuracies\n",
    "        df['val_IOU']  = val_accuracies\n",
    "        df['test_IOU']  = test_accuracies\n",
    "\n",
    "        df['train_IOU_neg']  = tr_accuracies_n\n",
    "        df['val_IOU_neg']  = val_accuracies_n\n",
    "        df['test_IOU_neg']  = test_accuracies_n\n",
    "\n",
    "        \n",
    "    #print(df['train_IOU'])\n",
    "        df.to_json(result_log_filename_json +\"_lstm\", orient ='records', lines = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
